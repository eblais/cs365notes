---
  title: 15. Space Complexity
  author: Eric Blais
  date: 2024-03-08
  layout: post
---

Time is not the only resource that is of interest in algorithms. Another important one is the amount of memory that algorithms require. 
The minimum amount of memory required to solve various computational problems can be studied through _space complexity_.


## Space Complexity Classes

The _space cost_ of the Turing machine $M$ on input $x$ is the total number of distinct tape cells that are visited by $M$'s tape head before $M$ halts. When $M$ is a multitape machine, we count the total number of visited cells in all the tapes to obtain the space cost.

> **Definition.**
> The _(worst-case) space cost_ of the Turing machine $M$ is the function $s : \mathbb{N} \to \mathbb{N}$ where $s(n)$ is the maximum space cost of $M$ on any input $x$ of length $|x| = n$.
{: .block-tip}

For every function $s : \mathbb{N} \to \mathbb{N}$, 
$\mathbf{SPACE}(s)$ is the set of all languages that can be decided by a Turing machine with worst-case space cost bounded above by $O(s)$; and

We can also extend the notion of space cost and space complexity classes to verifiers and to probabilistic Turing machines.
Doing so leads to analogues of the time complexity classes we have seen so far.


Space Class             | Definition
---                     | ---
$\mathbf{PSPACE}$   	| $$\bigcup_{k \ge 1} \mathbf{SPACE}(n^k)$$ 
$\mathbf{NPSPACE}$   	| All languages decidable by polynomial-space verifiers.
$\mathbf{BPPSPACE}$   	| All languages $\frac13$-decidable by polynomial-space probabilistic TMs.
$\mathbf{EXPSPACE}$  	| $$\bigcup_{k \ge 0} \mathbf{SPACE}(2^{n^k})$$


The class $\mathbf{PSPACE}$ contains $\mathbf{P}$ because a Turing machine that runs in polynomial time can only visit a polynomial number of distinct cells on the tape.
It also contains some problems that are not obviously solvable in polynomial time.

> **Proposition.**
> $\textsf{SAT} \in \mathbf{PSPACE}$.
{: .block-danger}

> **Proof.**
> Consider the simple exhaustive search algorithm for solving $\textsf{SAT}$. 
> Given a formula $\phi$, we start by checking to see if the assignment $x = 0^n$ satisfies it.
> If it does, we accept.
> Otherwise, we erase our work on the tape (leaving the encoding of $\phi$ and the assignment to $x$ that we just checked), increment the assignment, and repeat.
> If we have checked all the possible assignments of $x$ and none have satisfied$\phi$, then we halt and reject.
> 
> This simple algorithm decides $\textsf{SAT}$ because it accepts whenever $\phi$ has a satisfying assignment and rejects otherwise.
> And because it **reuses** the same memory for each assignment test, the space cost of this algorithm is the space required to check a single assignment, which is polynomial in the length of the encoding of $\phi$.

It even contains some languages that may not be in the polynomial hierarchy $\mathbf{PH}$.

> **Proposition.**
> The **T**otally **Q**uantified **B**oolean **F**ormula language
> 
> $$\textsf{TQBF} = \{ \left< \phi \right> : \exists x_1 \forall x_2 \cdots \exists/\forall x_n \mbox{ such that } \phi(x_1,x_2,\ldots,x_n) = 1\}$$
> 
> is in $\mathbf{PSPACE}$.
{: .block-danger}

> **Proof sketch.**
> The proof strategy is the same as the one for $\textsf{SAT}$: we iterate over all possible assignments of $x_1,\ldots,x_n$ to see if the condition on $\phi$ is satisfied.



## Time and Space

We can establish formal connections between time and space complexity classes in the following way.

> **Theorem.** _(Time-Space Hierarchy)_
> For every function $s : \mathbb{N} \to \mathbb{N}$, 
> 
> $$
> \mathbf{NTIME}(s) \subseteq \mathbf{SPACE}(s) \subseteq \mathbf{TIME}(2^{O(s)}).
> $$
{: .block-danger}

> **Proof.**
> We prove both inclusions separately with extensions of the universal Turing machine.
> 
> **$\mathbf{NTIME}(s) \subseteq \mathbf{SPACE}(s)$:**
> 
> Fix any $L \in \mathbf{NTIME}(s)$.
> Let $V$ be a verifier that decides $L$ and has time cost $O(s)$. 
> We can simulate all paths of length $O(s)$ with a deterministic Turing machine that iterates over all possible certificates $c$ to $V$ of length $s$.
> By erasing the contents of the simulation part of the tape between each simulation, we end up with overall space cost $O(s)$ since that is the space cost of $V$ and also an upper bound on the additional space we require for the simulation.
> And the resulting simulator accepts if and only if there is a certificate that causes $V$ to accept, so it decides $L$.
> 
> **$\mathbf{SPACE}(s) \subseteq \mathbf{TIME}(2^{O(s)})$:**
> 
> Fix any $L \in \mathbf{SPACE}(s)$.
> Let $M$ be any Turing machine with $m$ states and tape alphabet $\Gamma$ that decides $L$ and has space cost $O(s)$. 
> On any input of length $n$, $M$ can have at most 
> $m \cdot O(s) \cdot |\Gamma|^{O(s)}$ distinct configurations. 
> Since $M$ always terminates, it can never be in the same configuration twice in a computational path. 
> Thus, it must have time cost $t(n) = 2^{O(s(n))}$.


This theorem immediately lets us place $\mathbf{PSPACE}$ within the time complexity hierarchy.

> **Corollary.**
> $\mathbf{P} \subseteq \mathbf{NP} \subseteq \mathbf{PSPACE} \subseteq \mathbf{EXP}$.
{: .block-danger}

Since $\mathbf{P} \subsetneq \mathbf{EXP}$, at least one of the inclusions in the corollary is proper. It is believed that _all_ of them are proper, but so far all of them correspond to open problems. The first one would of course resolve the famous $\mathbf{P}$ vs. $\mathbf{NP}$ problem, but establishing either of the other two would also represent a significant breakthrough in complexity theory.


## Savitch's Theorem

The space complexity analogue of the $\mathbf{P}$ vs. $\sf NP$ problem is the natural question: Does $\mathbf{PSPACE} = \mathbf{NPSPACE}$? 

Remarkably, we _do_ have an answer to this question. Specifically, we know that any nondeterministic Turing machine can be simulated by a deterministic Turing machine with at most a quadratic increase in the amount of space required. This result is known as _Savitch's Theorem_.

To prove Savitch's Theorem, we consider the language DERIVE which includes all encodings $(\left< V \right>, x, c_1, c_2, t)$ of a verifier $V$, an input $x$ to $V$, two configurations $c_1$ and $c_2$ of $N$, and a value $t \in \mathbb{V}$ such that there exists a certificate $a$ to input $x$ that makes $V$ go from configuration $c_1$ to configuration $c_2$ by following at most $t$ transitions. 

> **Lemma.**
> There is a Turing machine that decides DERIVE and has  space cost only $O(s \log t)$ on input $(\left< V \right>, x, c_1, c_2, t)$ when $V$ has space cost $O(s)$ on input $x$.
{: .block-danger}

> **Proof.**
> The naive solution is to enumerate all paths of $t$ configurations of $V$ (over all choices of certificates) and check that they give a valid path from $c_1$ to $c_2$. Each configuration can be stored using $O(s)$ space, but the number of configurations is much too large: the space cost of the resulting Turing machine would be $O( s \cdot t)$.
> 
> We can obtain a much better space cost by applying the divide-and-conquer methodology. We can design a deterministic Turing machine that implements the following high-level algorithm:
> 
> * DERIVE $(c_1,c_2,t)$
>   * if $t=1$,
>     * Return ($c_1 = c_2)$ $\vee$ $(c_1 \vdash c_2$ in $N$)
>   * else
>     * for each configuration $c'$ of $V$
>       - if DERIVE$(c_1,c',t/2)$ and DERIVE$(c',c_2,t/2)$
>         * Return **True**
>     * Return **False**
> 
> The correctness of the Turing machine that implements the DERIVE algorithm follows from the fact that there is a path of length $t$ from $c_1$ to $c_2$ if and only if there exists some configuration $c'$ for which there are paths of length at most $t/2$ from $c_1$ to $c'$ and from $c'$ to $c_2$.
> 
> And the space savings come from the fact that a Turing machine implementing the DERIVE function with the algorithm above only needs to store the values of $c_1$, $c_2$, and $t$ for each instance of the algorithm on the current stack: since $t$ gets divided by $2$ in each call, the stack has depth at most $\log_2 t$, so in total the Turing machine can be implemented with space cost $O(s \cdot \log t)$.

We can now complete the proof of Savitch's theorem.

> **Theorem.** (_Savitch's Theorem_)
> For every $s : \mathbb{N} \to \mathbb{N}$,
> 
> $$
> \mathbf{NSPACE}\big(s(n)\big) \subseteq \mathbf{SPACE}\big(s(n)^2\big).
> $$
{: .block-danger}

> **Proof.**
> Fix any language $L \in \mathbf{NSPACE}(s)$.
> Let $V$ be a verifier that decides $L$ and has space cost $O(s(n))$.
> 
> A string $x$ is in $L$ if and only if there exists a sequence of consecutive configurations starting from the initial configuration $c_0$ of $V$ on input $x$ with some certificate $a$ and ending with an accepting configuration $c_{\bf A}$. 
> Without loss of generality, we can also assume that $c_{\bf A}$ is unique (say, by having $M$ erase the content of the tape before accepting).
> 
> As we have seen in the proof of the Time-Space Hierarchy Theorem, a Turing machine with space cost $O(s)$ can be in at most $2^{O(s)}$ distinct configurations. 
> So to determine if $V$ accepts $x$, we want to design a deterministic Turing machine $M$ that computes the problem DECIDE on input $(\left< V \right>, x, c_0, c_{\bf A}, t)$ for the value $t = 2^{O(s(n))}$.
> 
> By the last Lemma, such a machine $M$ for the DECIDE problem exists and has space complexity $O( s(n) \log t) = O(s(n)^2)$.

Savitch's Theorem immediately provides a solution to the $\mathbf{PSPACE}$ vs. $\mathbf{NPSPACE}$ question.

> **Corollary.**
> $\mathbf{PSPACE} = \mathbf{NPSPACE}.$
{: .block-danger}

Savitch's Theorem does leave one more fundamental open problem: is the quadratic blowup in space complexity required to go from nondeterministic to deterministic Turing machines? 



-----

_Eric Blais &copy;2024 &mdash; Last edited on Mar. 8, 2024_

